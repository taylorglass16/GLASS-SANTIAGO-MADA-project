---
title: "Statistical Analysis"
author: "Taylor Glass and Arlyn Santiago"
date: "03/12/24"
output: html_document
---
## Load necessary packages
```{r}
#| message: false
library(dplyr) # for basic syntax
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidymodels) # for model building 
library(flextable) # for creating tables for the manuscript
library(yardstick) # for finding performance metrics 
library(rsample) #for splitting the data into testing/training
library(knitr) # to create performance metric tables 
library(parsnip) # to create a workflow for cross-validation
library(glmnet) # for LASSO regression
library(ranger) # for random forest model 
```

## Load the cleaned data
```{r}
#path to data
data_location <- here("data","processed-data","processeddata.rds")
#load data
mydata <- readRDS(data_location)
```

## Data fitting and statistical analysis 
The outcome of interest in all of the following models is the percentage of unintended pregnancies associated with usage of short acting reversible contraception methods. The predictors used in our models include total number of women using short acting reversible methods standardized by population size, region, percent of women currently married, percent of women never married, total rate of maternal deaths, rate of maternal deaths due to unintended pregnancy, and rate of maternal deaths due to abortion.

### First Model Fit 
The first model attempts to predict the percentage of unintended pregnancies among women using short acting reversible methods of contraception 'pct_upreg_sarc' based on the total number of women using this method of contraception. We will use the version of the variable that has been standardized by population size, 'sarc_standard'. 
```{r}
# fit linear model using pct_upreg_sarc as outcome, sarc_standard as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit1 <- lm_mod %>% 
            fit(pct_upreg_sarc ~ sarc_standard, mydata) #estimate/train the model 

#generate clean output with estimates and p-values
tidy(lmfit1) 

# save results from fit into a data frame 
lmtable1 <- tidy(lmfit1)

# create a professional format of the table to add to the manuscript 
lmtable1 <- as_flextable(lmtable1)

# save fit results table  
table_file1 = here("results", "tables", "resulttable1.rds")
saveRDS(lmtable1, file = table_file1)
```
There appears to be a very strong relationship between total number of women using this birth control method and the outcome based on the coefficient estimate of 87.65998.

### Second Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, region as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit2 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_larcster ~ region, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit2) 

# save results from fit into a data frame 
lmtable2 <- tidy(lmfit2)

# create a professional format of the table to add to the manuscript 
lmtable2 <- as_flextable(lmtable2)

# save fit results table  
table_file2 = here("results", "tables", "resulttable2.rds")
saveRDS(lmtable2, file = table_file2)

```
This model shows that the Latin America/the Caribbean region has the strongest association with the outcome variable based on a coefficient estimate of 1.6218844 followed closely by the Asia region. The Oceania region has the weakest association with the outcome variable based on a coefficient estimate of 0.2512100.


### Third Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, pct_currentlymarried as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit3 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ pct_currentlymarried, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit3) 

# save results from fit into a data frame 
lmtable3 <- tidy(lmfit3)

# save fit results table  
table_file3 = here("results", "tables", "resulttable3.rds")
saveRDS(lmtable3, file = table_file3)
```
This model shows there is a negative relationship between the number of women currently married and the outcome. As the percentage of currently married women increases by 1, the percentage of unintended pregnancies decreases by a factor of 0.4181571.

# Fourth Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, pct_nevermarried as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit4 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ pct_nevermarried, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit4) 

# save results from fit into a data frame 
lmtable4 <- tidy(lmfit4)

# save fit results table  
table_file4 = here("results", "tables", "resulttable4.rds")
saveRDS(lmtable4, file = table_file4)
```
This model shows a positive relationship between the percentage of women never married and the outcome. As the percentage of women never married increases by 1, the percentage of unintended pregnancies increases by a factor of 0.2474218.  

# Fifth Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, rate_matdeaths as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit5 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ rate_matdeaths, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit5) 

# save results from fit into a data frame 
lmtable5 <- tidy(lmfit5)

# save fit results table  
table_file5 = here("results", "tables", "resulttable5.rds")
saveRDS(lmtable5, file = table_file5)
```
This model shows that total rate of maternal deaths is slightly negatively correlated with the outcome based on a small coefficient estimate of -0.01482568. 

# Sixth Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, rate_matdeaths_upreg as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit6 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ rate_matdeaths_upreg, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit6) 

# save results from fit into a data frame 
lmtable6 <- tidy(lmfit6)

# save fit results table  
table_file6 = here("results", "tables", "resulttable6.rds")
saveRDS(lmtable6, file = table_file6)
```
This model shows that total rate of maternal deaths due to unintended pregnancies has an even weaker association with the outcome based on a small coefficient estimate of -0.003158542. 

# Seventh Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, rate_matdeaths_abs as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit7 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ rate_matdeaths_abs, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit7) 

# save results from fit into a data frame 
lmtable7 <- tidy(lmfit7)

# save fit results table  
table_file7 = here("results", "tables", "resulttable7.rds")
saveRDS(lmtable7, file = table_file7)
```
This model shows that total rate of maternal deaths due to abortion has the strongest association with the outcome out of the three maternal mortality rates included based on a coefficient estimate of -0.033353527.

### Assessing correlation between predictors
There are some variables that are intuitively correlated, which means they should not be included in the same model. We want to explore those correlations before building our multivariate linear regression model. 
```{r}
# find correlation coefficients 
cor(mydata$pct_currentlymarried, mydata$pct_nevermarried)
cor(mydata$rate_matdeaths, mydata$rate_matdeaths_upreg)
cor(mydata$rate_matdeaths, mydata$rate_matdeaths_abs)
cor(mydata$rate_matdeaths_upreg, mydata$rate_matdeaths_abs)
```
The percent of currently married women and the percent of never married women have a correlation coefficient of -0.8588, so these two variables are highly correlated. The total rate of maternal deaths and rate of maternal deaths due to unintended pregnancies is the most highly correlated pair with a coefficient of 0.8904. The correlation between total rate of maternal deaths and rate of maternal deaths due to abortion is slightly lower with a coefficient of 0.8023. The two variables with the least correlation out of these four pairs are rate of maternal deaths due to unintended pregnancies and rate of maternal deaths due to abortion with a coefficient of 0.6486. Based on these associations, we will only include the percent of currently married women in the multivariate model because it has a slightly larger effect on the outcome with a more significant p-value in its single predictor model compared to the percent of never married women predictor. We will include both the rate of maternal deaths due to unintended pregnancies and rate of maternal deaths due to abortion because the total rate of maternal deaths is so highly correlated with each of these variables.

### Eighth Model Fit 
Based on the exploration of correlation between predictors, we include 5 predictors in the multiple linear regression model to determine how the associations interact. 
```{r}
# fit linear model using pct_upreg_sarc as outcome with all 7 predictors
lm_mod <- linear_reg() #specify the type of model
lmfit8 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ sarc_standard + region + pct_currentlymarried + rate_matdeaths_upreg + rate_matdeaths_abs, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit8) 

# save results from fit into a data frame 
lmtable8 <- tidy(lmfit8)

# create a professional format of the table to add to the manuscript 
lmtable8 <- as_flextable(lmtable8)

# save fit results table  
table_file8 = here("results", "tables", "resulttable8.rds")
saveRDS(lmtable8, file = table_file8)
```
This model shows the strongest predictor is the number of women using short acting reversible contraception with a huge coefficient of 85.6433. The next strongest predictor is the European region followed by the Latin America/the Caribbean region and the Asia region, which is a shift from the model with the singular region predictor. The Oceania region now has a negative association with the outcome. Both of the marriage status predictors are associated with a decrease in the outcome. The coefficient estimates for the rate of maternal death due to unintended pregnancies and abortion both flipped from the single predictor models to show a positive association with the outcome. 

### Nineth model fit 
It is important to create the null model for future assessment of model performance, which just calculates the mean outcome without any predictors present. 
```{r}
# create a null model
null_model <- null_model(mode = "regression") %>% 
    set_engine("parsnip") %>%
    fit(pct_upreg_sarc ~ 1, data = mydata)

#generate clean output with estimates and p-values
tidy(null_model) 

# save results from fit into a data frame 
nulltable <- tidy(null_model)

# save fit results table  
table_file9 = here("results", "tables", "resulttable9.rds")
saveRDS(nulltable, file = table_file9)
```
We also want to observe models with different combinations of predictors to determine if there is any evidence of interaction or effect measure modification. Multivariate analysis allows for the assessment of the association between each predictor and the outcome while adjusting for the effects of other variables. 

### Tenth Model fit 
We will create a model with all 3 rate of maternal mortality predictors to compare to the three models with each predictor on its own to further explore the correlation between these variables. 
```{r}
# fit linear model using pct_upreg_sarc as outcome with rate of maternal mortality predictors
lm_mod <- linear_reg() #specify the type of model
lmfit10 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ rate_matdeaths + rate_matdeaths_upreg + rate_matdeaths_abs, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit10) 

# save results from fit into a data frame 
lmtable10 <- tidy(lmfit10)

# save fit results table  
table_file10 = here("results", "tables", "resulttable10.rds")
saveRDS(lmtable10, file = table_file10)
```
It appears that there is an association between these three variables because the direction of the association between rate of maternal deaths and outcome has flipped. This model shows that rate of maternal deaths is associated with an increase in the outcome by a factor of 0.011267, while the single predictor model showed a slightly negatively correlation with the outcome based on a small coefficient estimate of -0.01482568. The other two variables retain their negative association with the outcome variable with similar coefficient measurements. 

## Assessing Model Performance
It is important to consider appropriate metrics to measure model performance. We will use RMSE and MAE at this point to compare 3 models: one with the number of women using short acting reversible contraception predictor, one using the percent currently married predictor, and one using the rate of maternal deaths due to abortion predictor. We chose these predictors because they have the strongest associations with the outcomes based on the single linear regression models shown above. We chose these performance metrics because they are commonly used for linear regression models like the ones we have created. RMSE provides an average magnitude of the error in the predictors, and MAE provides an average absolute difference between the predicted and actual values. We include both metrics to increase the validity of our results. 
```{r}
# set seed for reproducibility 
set.seed(2468)

# create predictions using the sarc_standard predictor model
poppred <- predict(lmfit1, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
popdata <- bind_cols(poppred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse1 <- rmse(popdata, truth = pct_upreg_sarc, estimate = .pred)
mae1 <- mae(popdata, truth = pct_upreg_sarc, estimate = .pred)

# create predictions using the pct_curmarried predictor model
marriedpred <- predict(lmfit3, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
marrieddata <- bind_cols(marriedpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse2 <- rmse(marrieddata, truth = pct_upreg_sarc, estimate = .pred)
mae2 <- mae(marrieddata, truth = pct_upreg_sarc, estimate = .pred)

# create predictions using the rate of maternal deaths due to abortion predictor model
matdeaths_abspred <- predict(lmfit7, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
matdeath_absdata <- bind_cols(matdeaths_abspred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse3 <- rmse(matdeath_absdata, truth = pct_upreg_sarc, estimate = .pred)
mae3 <- mae(matdeath_absdata, truth = pct_upreg_sarc, estimate = .pred)

# create predictions using the null model
nullpred <- predict(null_model, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
nulldata <- bind_cols(nullpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse4 <- rmse(nulldata, truth = pct_upreg_sarc, estimate = .pred)
mae4 <- mae(matdeath_absdata, truth = pct_upreg_sarc, estimate = .pred)


# create a matrix with RMSE and MAE values
results <- as.table(matrix(c(rmse1$.estimate, mae1$.estimate, rmse2$.estimate, mae2$.estimate, rmse3$.estimate, mae3$.estimate,
                             rmse4$.estimate, mae4$.estimate), nrow = 4, ncol = 2, byrow = TRUE))

# add column names
colnames(results) <- c("RMSE", "MAE")

# add row names
rownames(results) <- c("Model 1-Population Size Using SARCs", "Model 2-Currently Married", 
                             "Model 3-Rate of Maternal Deaths(Abortion)", "Model 4-Null")

# print the table
kable(results)

# save performance metrics results table  
table_file11 = here("results", "tables", "resulttable11.rds")
saveRDS(results, file = table_file11)
```

Both metrics favor the model with the total number of women using short acting reversible contraception methods based on the lowest RMSE value of 5.143319 and lowest MAE value of 3.953767. The model with the percentage of women currently married performs worse with an RMSE value of 10.17591 and MAE value of 8.107884. The same conclusion can be drawn for the model with the rate of maternal mortality due to abortions predictor only, given the RMSE value of 10.14128 and MAE value of 8.21687. The null model has an RMSE value of 9.842267 and MAE of 8.21687, so all models should have performance metrics lower than these values to be considered useful. 

It is important to determine if our model with the region predictor performs well because the main question to be answered by this analysis depends on this predictor. 
```{r}
# set seed for reproducibility 
set.seed(2468)

# create predictions using the sarc_standard predictor model
regionpred <- predict(lmfit2, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
regiondata <- bind_cols(regionpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse5 <- rmse(regiondata, truth = pct_upreg_sarc, estimate = .pred)
mae5 <- mae(regiondata, truth = pct_upreg_sarc, estimate = .pred)
print(rmse5)
print(mae5)
```

The results of these performance metrics are disappointing because 18.11857 is much larger than the null model's RMSE of 9.842267 and 14.66513 is also larger than the null model's MAE. However, this issue could possibly be explained by the fact that Africa has the most significant differences in our outcome compared to the other regions. We will still include this predictor in our multivariate regression model based on the significant Tukey conclusions and the large differences in outcome displayed in our exploratory data analysis results. 

After examining the relationship between the rate of maternal mortality variables, we want to determine which model performs the best. We have already found the RMSE value for the rate of maternal deaths due to abortion, which is stored as `rmse3`. We need to find the RMSE values for the other two single predictor models and the model with all 3 predictors.  
```{r}
# create predictions using the rate of maternal deaths predictor model
matdeathsallpred <- predict(lmfit5, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
matdeathalldata <- bind_cols(matdeathsallpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse6 <- rmse(matdeathalldata, truth = pct_upreg_sarc, estimate = .pred)
mae6 <- mae(matdeathalldata, truth = pct_upreg_sarc, estimate = .pred)

# create predictions using the rate of maternal deaths from unintended pregnancies predictor model
matdeaths_upregpred <- predict(lmfit6, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
matdeath_upregdata <- bind_cols(matdeaths_upregpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse7 <- rmse(matdeath_upregdata, truth = pct_upreg_sarc, estimate = .pred)
mae7 <- mae(matdeathalldata, truth = pct_upreg_sarc, estimate = .pred)


# create predictions using model with all three rate predictors
totalratepred <- predict(lmfit10, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
totalratedata <- bind_cols(totalratepred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse8 <- rmse(totalratedata, truth = pct_upreg_sarc, estimate = .pred)
mae8 <- mae(matdeathalldata, truth = pct_upreg_sarc, estimate = .pred)

# create a matrix with RMSE and MAE values
results2 <- as.table(matrix(c(rmse6$.estimate, mae6$.estimate, rmse7$.estimate, mae7$.estimate, rmse3$.estimate, mae3$.estimate, rmse8$.estimate, mae8$.estimate, rmse4$.estimate, mae4$.estimate), nrow = 5, ncol = 2, byrow = TRUE))

# add column names
colnames(results2) <- c("RMSE", "MAE")

# add row names
rownames(results2) <- c("Maternal Morality Model", "Maternal Mortality due to Unintended Pregnancies Model", 
                        "Maternal Mortality due to Abortion Model", "All 3 Maternal Mortality Predictors Model", "Null Model")

# print the table
kable(results2)

# save performance metrics results table  
table_file12 = here("results", "tables", "resulttable12.rds")
saveRDS(results2, file = table_file12)

```
The performance metrics show that the three single predictor models perform worse than the null model, which is not ideal. The model with all 3 maternal morality rate predictors performs exactly the same as the null model, which makes sense because the coefficients estimating the effect of each predictor on the outcome are so small. There is more variation in RMSE values compared to MAE values, but the conclusions are the same. 

We want to see how the model with all 5 predictors performs in comparison to the single predictor models. 
```{r}
# set seed for reproducibility 
set.seed(2468)

# create predictions using the multiple linear regression model
allpred <- predict(lmfit8, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
alldata <- bind_cols(allpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse9 <- rmse(alldata, truth = pct_upreg_sarc, estimate = .pred)
mae9 <- mae(alldata, truth = pct_upreg_sarc, estimate = .pred)
print(rmse9)
print(mae9)
```
As expected, this model performs better than the single predictor models with the lowest RMSE score of 4.361363 and lowest MAE score of 3.386193. 

We want to have a table comparing the RMSE and MAE values for the three single predictor models of interest along with the full model with all 5 predictors, which will all be compared to the null model. 
```{r}
# create a matrix with RMSE and MAE values
results3 <- as.table(matrix(c(rmse1$.estimate, mae1$.estimate, rmse2$.estimate, mae2$.estimate,
                             rmse3$.estimate, mae3$.estimate, rmse9$.estimate, mae9$.estimate,
                             rmse4$.estimate, mae4$.estimate), nrow = 5, ncol = 2, byrow = TRUE))

# add column names
colnames(results3) <- c("RMSE", "MAE")

# add row names
rownames(results3) <- c("Population Size Using SARCs", "Percent Currently Married", 
                       "Rate of Maternal Deaths due to Abortion", "Full Model with 5 Predictors", "Null Model")

# print the table
kable(results3)

# save performance metrics results table  
table_file14 = here("results", "tables", "resulttable14.rds")
saveRDS(results3, file = table_file14)
```

## Cross-Validation to Assess Model Performance
While the RMSE and MAE performance metrics are a good place to start for evaluating model performance, we need to determine how well the model performs on data it has not seen before to determine its usefulness. We will use the cross-validation process to do this. The first step is to split the data into testing and training portions. 
```{r}
# set seed for reproducibility
set.seed(2468)

# put 75% into the training data
datasplit <- initial_split(mydata, prop = 3/4)

# create data frames for the two sets
train_data <- training(datasplit)
test_data <- testing(datasplit)
```

We will only cross-validate the single predictor model with the highest performance, `lmfit1`, with the total population size using short acting reversible contraception and the  multivariate regression model,`lmfit8`, because it performs the best in terms of RMSE and MAE. Our dataset is relatively small with only 132 observations, so we will use 5 folds for the cross-validation process.  
```{r}
# set seed for reproducibility
set.seed(2468)

# create 10 folds
folds <- vfold_cv(train_data, v=5)

# define the model specification
lm_spec <- linear_reg() %>% 
  set_engine("lm")


# initialize a workflow and fit it to 'lmfit1'
my_workflow <- workflow() %>%
  add_model(lm_spec) %>% 
  add_formula(pct_upreg_sarc ~ sarc_standard) %>% #specify the formula since we are not using a recipe
  fit(data = train_data) 

# resample the model 
my_resample <- my_workflow %>% 
                fit_resamples(folds)

# initialize a workflow and fit it to 'lmfit8'
my_workflow2 <- workflow() %>%
  add_model(lm_spec) %>% 
  add_formula(pct_upreg_sarc ~ sarc_standard + region + pct_currentlymarried + rate_matdeaths_upreg + rate_matdeaths_abs) %>% #specify the formula since we are not using a recipe
  fit(data = train_data) 

# resample the model 
my_resample2 <- my_workflow2 %>% 
                fit_resamples(folds)

# collect the metrics from both models
collect_metrics(my_resample)
collect_metrics(my_resample2)
```
The cross-validated results lead to the same conclusion: the multiple linear regression model with 5 predictors performs better. The RMSE value for the single predictor model with `sarc_standard` is 4.822226, which is lower the the first RMSE value of 5.143319. While the RMSE value for the multiple linear regression model is slightly higher than the first time it was calculated, it is still slightly lower at 4.405716. Additionally, the standard error of the RMSE for the multiple linear regression model is 0.2768, which is lower than the standard error of the RMSE for the single predictor model of 0.3528. This means there is higher variance in the single predictor model. The R-squared value of 0.8206 for the multiple linear regression model is higher than 0.7861 for the single predictor model, which means the multiple linear regression model accounts for more of the variability in the data. Overall, the multiple linear regression model performs better.

We want a table displaying these results as well. 
```{r}
# create a matrix with RMSE and MAE values
results4 <- as.table(matrix(c(4.822226, 0.25279125, 0.786067, 0.04292202, 4.405716, 0.27679502, 0.820600, 0.02815933), nrow = 2, ncol = 4, byrow = TRUE))

# add column names
colnames(results4) <- c("RMSE", "Standard Error", "R-Squared", "Standard Error")

# add row names
rownames(results4) <- c("Single Predictor Model", "Full Predictor Model")

# print the table
kable(results4)

# save performance metrics results table  
table_file15 = here("results", "tables", "resulttable15.rds")
saveRDS(results4, file = table_file15)
```

Since we have concluded that the model with all 5 predictors performs the best, we will use this model to make predictions for the test data. We will create a graph to show how well the model performed on the training and testing data. 
```{r}
# set seed for reproducibility
set.seed(2468)

# create predictions using MLR model with the test data
testpred <- predict(my_workflow2, new_data = test_data) %>% 
                  select(.pred)

# create predictions using MLR model with training data
trainpred <- predict(my_workflow2, new_data = train_data) %>% 
                  select(.pred)

# create data frame with the observed values predicted values from model 2
plotdata <- data.frame(
  observed = c(train_data$pct_upreg_sarc), 
  training_model2 = c(trainpred$.pred), 
  test_model2 = c(testpred$.pred),
  model = rep(c("Observed", "Training Model", "Testing Model"), each = nrow(train_data))) # add label indicating the model

# create a visual representation with 2 sets of predicted and observed values 
figure1 <- ggplot(plotdata, aes(x = observed)) +
              geom_point(aes(y = training_model2, color = "training model"), shape = 3) +
              geom_point(aes(y = test_model2, color = "testing model"), shape = 3) +
              geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") + #add the 45               degree line
              xlim(0, 50) + #limit the x-axis values
              ylim(0, 50) + #limit the y-axis values
              labs(x = "Observed Values", y = "Predicted Values", title = "Comparing Training and Testing Data on MLR Model")
figure1

# save final figure
figure_file = here("results", "figures", "final-figures", "fullmodelpred.png")
ggsave(filename = figure_file, plot=figure1) 
```
The training model predictions, shown in teal, hover pretty closely around the expected relationship between the outcome and the 5 predictors, which is depicted as the 45 degree line. The testing model predictions, shown in coral, are much more extreme than the training model predictions because they are farther from the 45 degree line. While there is some overlap between the two sets of predictions, it is likely that our model is overfitting the data because the testing observations are so far from the expected relationship. 

To examine the uncertainty in the model further, we will use the bootstrapping method to resample our data 100 times to find confidence intervals for our predictions. 
```{r}
# set the seed for reproducibility
set.seed(2468)

# create 100 bootstraps with the training data
bootstraps <- bootstraps(train_data, times = 100)

# create empty vector to store predictions list 
preds_bs <- vector("list", length = length(bootstraps))

# write a loop to fit the model to each bootstrap and make predictions
for (i in 1:length(bootstraps)) {
  
  bootstrap_sample <- analysis(bootstraps$splits[[i]])  # isolate the bootstrap sample
  
  model <- lm(pct_upreg_sarc ~ sarc_standard + region + pct_currentlymarried + rate_matdeaths_upreg + rate_matdeaths_abs, data = bootstrap_sample) # fit the model using the bootstrap sample
  
  predictions <- predict(model, newdata = train_data) # make predictions with the training data
  
  preds_bs[[i]] <- predictions # store predictions in the empty vector
}
# create an empty array to store the predictions
num_samples <- length(preds_bs)
num_datapoints <- length(preds_bs[[1]])  
preds_array <- array(NA, dim = c(num_samples, num_datapoints))

# fill the array with predictions from bootstrappping
for (i in 1:num_samples) {
  preds_array[i,] <- unlist(preds_bs[[i]])
}

# find the median and 95% confidence intervals of each prediction
preds <- preds_array %>%
          apply(2, quantile,  c(0.05, 0.5, 0.95)) %>%
          t()

# create a data frame with all the necessary variables
finalbootstrap <- data.frame(
  observed = c(train_data$pct_upreg_sarc), 
  point_estimates = c(trainpred$.pred),
  medians = preds[, 2],
  lower_bound = preds[, 1],
  upper_bound = preds[, 3]
)

# create visualization with all 5 variables 
figure2 <- ggplot(finalbootstrap, aes(x = observed, y = point_estimates)) +
              geom_point(aes(shape = "Point Estimates", color = "black")) +  
              geom_point(aes(y = medians, shape = "Medians",color = "coral")) + 
              geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), width = 0.1, color = "blue") + 
              geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +  # add 45 degree line
              xlim(0, 50) + # limit the x-axis values
              ylim(0, 50) + # limit the y-axis values
              labs(x = "Observed Values", y = "Predicted Values", title = "Uncertainty in Predictions with Final Model") +
              scale_color_manual(values = c("black" = "black", "coral" = "coral", "blue" = "blue"),
                                 labels = c("Point Estimates", "Medians", "Error Bars"))
figure2

# save final figure
figure_file2 = here("results", "figures", "final-figures", "bootstrappred.png")
ggsave(filename = figure_file2, plot=figure2) 
```
This figure shows that 95% confidence intervals, shown as dark blue lines, for the predictions that do not fall on the expected 45 degree line do not intersect with the line either. The point estimates, represented as black triangles, have decent overlap with the medians, represented as coral circles, which is a sign of good agreement between predicted and observed values. The dark blue lines show that the confidence intervals are narrow, which increases the validity of the conclusion that the model is overfitting the data because there is not much uncertainty in the predictions. Unfortunately, the model is inaccurate, but it is inaccurate without much variation. 

Since we have evidence that our model overfits the data, we want to explore our full model using Lasso regression. This regression analysis shrinks the coefficients towards the mean, which allows it to perform subset selection. Coefficients can reach zero, and that results in that variable being removed from the model. This technique could reveal predictors that should not be included in our final model, which could reduce the risk of overfitting. 
```{r}
# set seed for reproducibility
set.seed(2468)

# LASSO regression workflow
lasso_mod <- linear_reg(penalty = 0.1, mixture = 1, mode = "regression") %>% #set the penalty
                set_engine("glmnet") 

lasso_results <- lasso_mod %>% 
            fit(pct_upreg_sarc ~ sarc_standard + region + pct_currentlymarried + rate_matdeaths_upreg +
                  rate_matdeaths_abs, data = mydata) 

# print results 
tidy(lasso_results) 

# save results from fit into a data frame 
lmtable11 <- tidy(lasso_results)

# create a professional format of the table to add to the manuscript 
lmtable11 <- as_flextable(lmtable11)

# save fit results table  
table_file16 = here("results", "tables", "resulttable16.rds")
saveRDS(lmtable11, file = table_file16)
```
As expected, the coefficients are much smaller than the first full model we created without the regularization penalty, but all 5 predictors remain in the model. The `sarc_standard` predictor still has the strongest relationship with the outcome. The percent of women currently married, rate of maternal deaths due to unintended pregnancies, and rate of maternal deaths to due to abortion all have weak negative associations with the outcome. The direction of the relationship between the rate of maternal deaths due to abortions and the outcome flipped from the first full regression model to this model, but the coefficient is so small that this difference is not significant.  

It is important to tune the hyperparameters associated with LASSO regression to determine which model fits best. We will use a grid search to ensure that we try all combinations of the parameters in our grid. We will complete the tuning process with a 5-fold cross-validation repeated 5 times. 
```{r}
# set seed for reproducibility
set.seed(2468)

# repeat lasso model workflow
lasso_mod2 <- linear_reg(penalty = tune(), mixture = 1, mode = "regression") %>% # allow for tuning
                set_engine("glmnet") 

# change variable type of region for tune_grid()
mydata$region <- as.numeric(mydata$region)

# define the recipe
recipe <- recipe(pct_upreg_sarc ~ sarc_standard + region + pct_currentlymarried + rate_matdeaths_upreg +
                  rate_matdeaths_abs, data = mydata) 

# define the workflow
workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(lasso_mod2)

# define a parameter grid with 50 values linearly spaced on a log scale
model_grid <- grid_regular(penalty(range = c(log10(1e-5), log10(1e2))), levels = 50)

# create the real samples cross-validation 
real_samples <- vfold_cv(mydata, v = 5, repeats = 5)

# tune the model 
tuned_model <- workflow %>% 
                tune_grid(resamples = real_samples,
                          grid = model_grid,
                          metrics = metric_set(rmse))

# observe the tuning process
figure3 <- tuned_model %>% 
             autoplot()
figure3

# save the final figure 
figurefile3 <- here("results", "figures", "final-figures", "lassoparameters.png")
ggsave(filename = figurefile3, plot = figure3)
```
As the regularization penalty increases past 1, we see that the RMSE value increases exponentially. The lowest RMSE for the model is slightly greater than 5 when the regularization penalty is less than 1. When the regularization penalty increases to 10, the RMSE increases to almost 11. These observations are expected because the RMSE can only increase in LASSO regression because the penalty parameter is added to the SSR term to determine the value of the cost function. It is expected that the model will perform worse according to the RMSE scale as the penalty parameter increases. This plot shows that penalty parameters less than 1 are preferable for model performance. Since we originally chose 0.1 as our penalty parameter, we found one of the best performing models with the LASSO regression.

We also want to explore a random forest model with all 5 main predictors since the previous cross-validation process revealed high overfitting of the multivariate linear regression model. We will use RMSE, MAE, and R squared as performance metrics to compare it to the previous LASSO regression model. 
```{r}
#| message: false
# set seed for reproducibility
set.seed(2468)

# create workflow for random forest model
rf_mod <- rand_forest(mode = "regression") %>% # specify the model type
            set_engine("ranger", seed = 2468) #  set seed for reproducibility internally

random_forest <- rf_mod %>% 
                    fit(pct_upreg_sarc ~ sarc_standard + region + pct_currentlymarried + rate_matdeaths_upreg +
                    rate_matdeaths_abs, data = mydata) # include all predictors

# create predictions using the random forest model
RFpred <- predict(random_forest, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
RFdata <- bind_cols(allpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse10 <- rmse(RFdata, truth = pct_upreg_sarc, estimate = .pred)
mae10 <- mae(RFdata, truth = pct_upreg_sarc, estimate = .pred)
rsq10 <- rsq(RFdata, truth = pct_upreg_sarc, estimate = .pred)

# print performance metric results 
print(rmse10)
print(mae10)
print(rsq10)

# visualize random forest predictions verus observed values 
figure4 <-ggplot(RFdata, aes(x=.pred, y= pct_upreg_sarc)) + 
                geom_point() +
                geom_abline(linetype = "dashed") + 
                labs(x="Predicted Values", y="Observed Values", title="Random Forest Model Predictions and Observations")
figure4

# save the random forest figure 
figurefile4 <- here("results", "figures", "final-figures", "randomforestpred.png")
ggsave(filename = figurefile4, plot = figure4)
```
The random forest model has an RMSE of 4.361363, an MAE of 3.386193, and an R-squared of 0.8369754. Compared to the previous model with an RMSE of 4.361363, an MAE of 3.386193, and an R-squared of 0.820600, the random forest model performs very similarly. The RMSE and MAE values are exactly the same because the same 5 predictors are used for the outcome, but the R-squared value shows that the random forest model performs slightly better. Since a higher R-squared value represents smaller differences between the observed and predicted values, we can conclude that this model is predicting our outcome with slightly higher accuracy. The plot of the predicted versus observed values shows strong correlation for predicted values less than 15, but the deviation from the expected values increases as the predicted values increase. 

It is crucial to tune the parameters associated with the random forest model, so we will do that before concluding that it outperforms the LASO regression model. We updated the workflow to allow for tuning of 2 variables: `mtry`, the number of variable to possibly split at in each node, and `min_n`, the minimal node size. We set the number of trees to 300 because it strikes a good balance between achieving the best performance metrics and the processing time required to do so. 
```{r}
#| message: false
# set seed for reproducibility
set.seed(2468)

# define the recipe
recipe <- recipe(pct_upreg_sarc ~ sarc_standard + region + pct_currentlymarried + rate_matdeaths_upreg +
                    rate_matdeaths_abs, data = mydata) 

# update workflow accordingly 
rf_mod2 <- rand_forest(mode = "regression",
                      mtry = tune(),  # allow for tuning
                      min_n = tune(), # allow for tuning
                      trees = 300) %>% 
                          set_engine("ranger", seed = 2468) 

# use updated workflow
RFworkflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(rf_mod2) 

# define a 5 by 5 parameter grid
RFparameter_grid <- grid_regular(mtry(range = c(1,5)),
                            min_n(range = c(1,15)),
                            levels = 5)

# create the real samples cross-validation 
real_samples <- vfold_cv(mydata, v = 5, repeats = 5)

# tune the random forest model 
tuned_RFmod <- RFworkflow %>% 
                    tune_grid(resamples = real_samples,
                          grid = RFparameter_grid,
                          metrics = metric_set(rmse))

# observe the tuning process
rf_tune_plot <- tuned_RFmod %>% 
              autoplot()
rf_tune_plot

# save plot 
figure_file5 = here("results", "figures", "final-figures", "randomforesttune.png")
ggsave(filename = figure_file5, plot=rf_tune_plot) 

```
Based on the red line that shows the lowest possible RMSE score, our random forest model performs the best with a node size of 1 and 4 predictors. This begs us to ask the question of which one of the 5 predictors included in our model should be dropped to improve model performance because all previous performance metrics suggested that including all 5 predictors created the best model. 


## Choosing a final model 
After considering each of the 10 models we created throughout the statistical analysis process, the random forest model with 5 predictors is the best overall choice for predicting our outcome of percentage of unintended pregnancies due to short acting reversible methods. The collinearity between the total rate of maternal deaths, rate of maternal deaths due to abortion, and rate of maternal deaths due to unintended pregnancies justified why they should not be included in the same model. We chose to remove the total maternal mortality predictor to avoid the issue of collinearity because the other two predictors in this category had lower RMSE and MAE performance metrics. The same reasoning explains why the percent of currently married women and the percent of never married women predictors should not be included in the same model. We chose to include the percent of currently married women in the final model because it had the strongest relationship with the outcome. The four most promising models were the single predictor models with population size using short acting reversible contraception, percent of currently married women, and rate of maternal deaths due to abortion, respectively, along with the full model with all five predictors. The single model with the region variable only performed poorly with an RMSE of 18.11857, so it was not considered in the final running for the best model. The models with percent of currently married women and rate of maternal deaths due to abortion had comparable RMSE scores of 10.175909 and 10.141275, respectively. Large improvement in performance metrics was observed when considering the model with population size using short acting reversible contraception based on an RMSE of 5.143319. The full model with all five predictors, including short acting reversible contraception usage, region, percent of currently married women, rate of maternal mortality due to abortion, and rate of maternal mortality due to unintended pregnancies, performed the best with an RMSE of 4.361363. We felt it was important to include the region predictor in the final model even though it did not perform well in a model on its own because of the large differences in the coefficients for the region predictor in a linear model with the outcome. Latin America/the Caribbean and Asia have much larger effects on the outcome than Europe and Oceania, so we argue for the inclusion of this variable in the final model to capture this information. 

Once we decided that the full model with 5 predictors was the best choice, we used cross-validation with LASSO regression and random forest modeling to explore subset selection. The LASSO regression kept all 5 predictors, and the coefficients for the Asia, Europe, and Latin America/the Caribbean predictors were the largest behind the sarc_standard predictor. The smallest coefficient was assigned to the percent of currently married women predictor. The parameters were tuned for the LASSO regression, which increased our confidence in using a penalty of 0.1. The random forest model generated the same RMSE of 4.361363 as the LASSO regression model, but it has a higher R-squared value of 0.8369754. Since the random forest model explains 83.69% of the variation between the predicted and observed outcomes compared to 82.06% explained by the LASSO regression model, it is slightly better at predicting the outcome. After tuning the parameters for this model, the number of randomly selected predictors that produces the lowest RMSE is 4, not 5. While we cannot be sure which predictor the model is suggesting to remove, we hypothesize that it is the percent of currently married women based on the information from the LASSO regression. However, the improvement in RMSE when using 4 predictors instead of 5 is minimal, but it is interesting that the graph suggests using a minimal node size of 4 if the model contains all 5 predictors. Based on all of this information, we suggest using the full model with all 5 predictors with a LASSO regression because it accurately portrays the importance of each predictor while providing a penalty to predictors that do not meaningfully contribute to the model. While this model has a slightly lower R-squared value compared to the random forest model, the evidence of overfitting is so strong that this difference is negligible. Cross-validation of the model with all 5 predictors showed that predictions from testing data deviated from the expected relationship with larger variance than the predictions from the training data. Since the model did not perform well when used on test data, we conclude that there is overfitting in our model. Unfortunately, it is likely that any of these models are not optimal choices for predicting our outcome based on the poor performance of our model on testing data. While we suggest using the LASSO regression with all 5 predictors to predict our outcome because it is slightly easier to interpret, future researchers who are willing to sacrifice interpretation for predictive power should use the random forest model with 4 predictors based on the slightly higher predictive performance shown by the R-squared performance metric. 